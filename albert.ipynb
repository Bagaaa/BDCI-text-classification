{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pickle\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(result):\n",
    "    df = pd.DataFrame([result]).T\n",
    "    df.columns = [\"values\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer_from_hub_module(bert_model_hub):\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(bert_model_hub)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\",\n",
    "                                        as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([\n",
    "                tokenization_info[\"vocab_file\"],\n",
    "                tokenization_info[\"do_lower_case\"]\n",
    "            ])\n",
    "\n",
    "    return bert.tokenization.FullTokenizer(vocab_file=vocab_file,\n",
    "                                           do_lower_case=do_lower_case)\n",
    "\n",
    "\n",
    "def make_features(dataset, label_list, MAX_SEQ_LENGTH, tokenizer, DATA_COLUMN,\n",
    "                  LABEL_COLUMN):\n",
    "    input_example = dataset.apply(lambda x: bert.run_classifier.InputExample(\n",
    "        guid=None, text_a=x[DATA_COLUMN], text_b=None, label=x[LABEL_COLUMN]),\n",
    "                                  axis=1)\n",
    "    features = bert.run_classifier.convert_examples_to_features(\n",
    "        input_example, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "    return features\n",
    "\n",
    "\n",
    "# creating model, replace it by custom model\n",
    "def create_model(bert_model_hub, is_predicting, input_ids, input_mask,\n",
    "                 segment_ids, labels, num_labels):\n",
    "    \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "    bert_module = hub.Module(bert_model_hub, trainable=True)\n",
    "    bert_inputs = dict(input_ids=input_ids,\n",
    "                       input_mask=input_mask,\n",
    "                       segment_ids=segment_ids)\n",
    "    bert_outputs = bert_module(inputs=bert_inputs,\n",
    "                               signature=\"tokens\",\n",
    "                               as_dict=True)\n",
    "\n",
    "    # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "    # Use \"sequence_outputs\" for token-level output.\n",
    "    output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    # Create our own layer to tune for politeness data.\n",
    "    output_weights = tf.get_variable(\n",
    "        \"output_weights\", [num_labels, hidden_size],\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.get_variable(\"output_bias\", [num_labels],\n",
    "                                  initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "\n",
    "        # Dropout helps prevent overfitting\n",
    "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "        # Convert labels into one-hot encoding\n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "        predicted_labels = tf.squeeze(\n",
    "            tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "        # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "        if is_predicting:\n",
    "            return (predicted_labels, log_probs)\n",
    "\n",
    "        # If we're train/eval, compute loss between predicted and actual label\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        return (loss, predicted_labels, log_probs)\n",
    "\n",
    "\n",
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(bert_model_hub, num_labels, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps):\n",
    "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "\n",
    "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "\n",
    "        # TRAIN and EVAL\n",
    "        if not is_predicting:\n",
    "\n",
    "            (loss, predicted_labels,\n",
    "             log_probs) = create_model(bert_model_hub, is_predicting,\n",
    "                                       input_ids, input_mask, segment_ids,\n",
    "                                       label_ids, num_labels)\n",
    "\n",
    "            train_op = bert.optimization.create_optimizer(loss,\n",
    "                                                          learning_rate,\n",
    "                                                          num_train_steps,\n",
    "                                                          num_warmup_steps,\n",
    "                                                          use_tpu=False)\n",
    "\n",
    "            # Calculate evaluation metrics.\n",
    "            def metric_fn(label_ids, predicted_labels):\n",
    "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "                f1_score = tf.contrib.metrics.f1_score(label_ids,\n",
    "                                                       predicted_labels)\n",
    "                auc = tf.metrics.auc(label_ids, predicted_labels)\n",
    "                recall = tf.metrics.recall(label_ids, predicted_labels)\n",
    "                precision = tf.metrics.precision(label_ids, predicted_labels)\n",
    "                true_pos = tf.metrics.true_positives(label_ids,\n",
    "                                                     predicted_labels)\n",
    "                true_neg = tf.metrics.true_negatives(label_ids,\n",
    "                                                     predicted_labels)\n",
    "                false_pos = tf.metrics.false_positives(label_ids,\n",
    "                                                       predicted_labels)\n",
    "                false_neg = tf.metrics.false_negatives(label_ids,\n",
    "                                                       predicted_labels)\n",
    "                return {\n",
    "                    \"eval_accuracy\": accuracy,\n",
    "                    \"f1_score\": f1_score,\n",
    "                    \"auc\": auc,\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"true_positives\": true_pos,\n",
    "                    \"true_negatives\": true_neg,\n",
    "                    \"false_positives\": false_pos,\n",
    "                    \"false_negatives\": false_neg\n",
    "                }\n",
    "\n",
    "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                                  loss=loss,\n",
    "                                                  train_op=train_op)\n",
    "            else:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                                  loss=loss,\n",
    "                                                  eval_metric_ops=eval_metrics)\n",
    "        else:\n",
    "            (predicted_labels,\n",
    "             log_probs) = create_model(bert_model_hub, is_predicting,\n",
    "                                       input_ids, input_mask, segment_ids,\n",
    "                                       label_ids, num_labels)\n",
    "\n",
    "            predictions = {\n",
    "                'probabilities': log_probs,\n",
    "                'labels': predicted_labels\n",
    "            }\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # Return the actual model function in the closure\n",
    "    return model_fn\n",
    "\n",
    "\n",
    "def estimator_builder(bert_model_hub, OUTPUT_DIR, SAVE_SUMMARY_STEPS,\n",
    "                      SAVE_CHECKPOINTS_STEPS, label_list, LEARNING_RATE,\n",
    "                      num_train_steps, num_warmup_steps, BATCH_SIZE):\n",
    "\n",
    "    # Specify outpit directory and number of checkpoint steps to save\n",
    "    run_config = tf.estimator.RunConfig(\n",
    "        model_dir=OUTPUT_DIR,\n",
    "        save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "        save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
    "\n",
    "    model_fn = model_fn_builder(bert_model_hub=bert_model_hub,\n",
    "                                num_labels=len(label_list),\n",
    "                                learning_rate=LEARNING_RATE,\n",
    "                                num_train_steps=num_train_steps,\n",
    "                                num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "    estimator = tf.estimator.Estimator(model_fn=model_fn,\n",
    "                                       config=run_config,\n",
    "                                       params={\"batch_size\": BATCH_SIZE})\n",
    "    return estimator, model_fn, run_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_dfs(\n",
    "        train,\n",
    "        test,\n",
    "        DATA_COLUMN,\n",
    "        LABEL_COLUMN,\n",
    "        MAX_SEQ_LENGTH=128,\n",
    "        BATCH_SIZE=32,\n",
    "        LEARNING_RATE=2e-5,\n",
    "        NUM_TRAIN_EPOCHS=3.0,\n",
    "        WARMUP_PROPORTION=0.1,\n",
    "        SAVE_SUMMARY_STEPS=100,\n",
    "        SAVE_CHECKPOINTS_STEPS=10000,\n",
    "        bert_model_hub=\"https://tfhub.dev/google/bert_chinese_L-12_H-768_A-12/1\"\n",
    "):\n",
    "\n",
    "    label_list = train[LABEL_COLUMN].unique().tolist()\n",
    "\n",
    "    tokenizer = create_tokenizer_from_hub_module(bert_model_hub)\n",
    "\n",
    "    train_features = make_features(train, label_list, MAX_SEQ_LENGTH,\n",
    "                                   tokenizer, DATA_COLUMN, LABEL_COLUMN)\n",
    "    test_features = make_features(test, label_list, MAX_SEQ_LENGTH, tokenizer,\n",
    "                                  DATA_COLUMN, LABEL_COLUMN)\n",
    "\n",
    "    num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "    num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "    estimator, model_fn, run_config = estimator_builder(\n",
    "        bert_model_hub, OUTPUT_DIR, SAVE_SUMMARY_STEPS, SAVE_CHECKPOINTS_STEPS,\n",
    "        label_list, LEARNING_RATE, num_train_steps, num_warmup_steps,\n",
    "        BATCH_SIZE)\n",
    "\n",
    "    train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "        features=train_features,\n",
    "        seq_length=MAX_SEQ_LENGTH,\n",
    "        is_training=True,\n",
    "        drop_remainder=False)\n",
    "\n",
    "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "\n",
    "    test_input_fn = run_classifier.input_fn_builder(features=test_features,\n",
    "                                                    seq_length=MAX_SEQ_LENGTH,\n",
    "                                                    is_training=False,\n",
    "                                                    drop_remainder=False)\n",
    "\n",
    "    result_dict = estimator.evaluate(input_fn=test_input_fn, steps=None)\n",
    "    return result_dict, estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8921</th>\n      <td>0</td>\n      <td>老板超级好，后面麻的我受不了啦，做死帮我们想办法，服务员态度超级好，会耐心介绍，真心推荐，但...</td>\n    </tr>\n    <tr>\n      <th>3765</th>\n      <td>0</td>\n      <td>味道不错，下次还来</td>\n    </tr>\n    <tr>\n      <th>1679</th>\n      <td>0</td>\n      <td>买了些素菜，味道特别好，就是太辣了</td>\n    </tr>\n    <tr>\n      <th>4217</th>\n      <td>0</td>\n      <td>难吃，真的难吃，好坑</td>\n    </tr>\n    <tr>\n      <th>5591</th>\n      <td>1</td>\n      <td>日你妈&amp;nbsp;饭有头发&amp;nbsp;木耳上的壳壳都在&amp;nbsp;珍珠奶茶没有珍珠&amp;nbsp;</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "      label                                            comment\n8921      0  老板超级好，后面麻的我受不了啦，做死帮我们想办法，服务员态度超级好，会耐心介绍，真心推荐，但...\n3765      0                                          味道不错，下次还来\n1679      0                                  买了些素菜，味道特别好，就是太辣了\n4217      0                                         难吃，真的难吃，好坑\n5591      1    日你妈&nbsp;饭有头发&nbsp;木耳上的壳壳都在&nbsp;珍珠奶茶没有珍珠&nbsp;"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./data/safety_train_valid.pickle\", 'rb') as f:\n",
    "    train, test = pickle.load(f)\n",
    "\n",
    "train = train.sample(len(train))\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "myparam = {\n",
    "    \"DATA_COLUMN\": \"comment\",\n",
    "    \"LABEL_COLUMN\": \"label\",\n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"LEARNING_RATE\": 2e-5,\n",
    "    \"NUM_TRAIN_EPOCHS\": 3,\n",
    "    \"bert_model_hub\": \"/Users/kevin/Downloads/albert_xlarge_zh_183k\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No such graph variant: tags=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-88e17f0179f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_on_dfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmyparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-e487c6102fe3>\u001b[0m in \u001b[0;36mrun_on_dfs\u001b[0;34m(train, test, DATA_COLUMN, LABEL_COLUMN, MAX_SEQ_LENGTH, BATCH_SIZE, LEARNING_RATE, NUM_TRAIN_EPOCHS, WARMUP_PROPORTION, SAVE_SUMMARY_STEPS, SAVE_CHECKPOINTS_STEPS, bert_model_hub)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mlabel_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLABEL_COLUMN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tokenizer_from_hub_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_model_hub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     train_features = make_features(train, label_list, MAX_SEQ_LENGTH,\n",
      "\u001b[0;32m<ipython-input-4-344213e8b115>\u001b[0m in \u001b[0;36mcreate_tokenizer_from_hub_module\u001b[0;34m(bert_model_hub)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mbert_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_model_hub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         tokenization_info = bert_module(signature=\"tokenization_info\",\n\u001b[1;32m      6\u001b[0m                                         as_dict=True)\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.2/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spec, trainable, name, tags)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tags\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m       \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No such graph variant: tags=%r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0mabs_state_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_get_state_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmark_name_scope_used\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No such graph variant: tags=None"
     ]
    }
   ],
   "source": [
    "result, estimator = run_on_dfs(train, test, **myparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrediction(in_sentences, estimator):\n",
    "    labels = [\"Negaitive\", \"Positive\"]\n",
    "    label_list = train['label'].unique().tolist()\n",
    "    MAX_SEQ_LENGTH = 128\n",
    "    bert_model_hub = \"/Users/kevin/Downloads/albert_xlarge_zh_183k\"\n",
    "\n",
    "    tokenizer = create_tokenizer_from_hub_module(bert_model_hub)\n",
    "    input_examples = [\n",
    "        bert.run_classifier.InputExample(guid=None,\n",
    "                                         text_a=x,\n",
    "                                         text_b=None,\n",
    "                                         label=0) for x in in_sentences\n",
    "    ]\n",
    "    input_features = bert.run_classifier.convert_examples_to_features(\n",
    "        input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "    predict_input_fn = bert.run_classifier.input_fn_builder(\n",
    "        features=input_features,\n",
    "        seq_length=MAX_SEQ_LENGTH,\n",
    "        is_training=False,\n",
    "        drop_remainder=False)\n",
    "\n",
    "    predictions = estimator.predict(predict_input_fn)\n",
    "\n",
    "    return [(sentence, prediction['probabilities'],\n",
    "             labels[prediction['labels']])\n",
    "            for sentence, prediction in zip(in_sentences, predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv('test_new.csv')\n",
    "pred_sentences = target['comment'].values.tolist()\n",
    "predictions = getPrediction(pred_sentences, estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "target['label'] = None\n",
    "for row in range(len(predictions)):\n",
    "    if predictions[row][2] == 'Positive':\n",
    "        target.loc[row, 'label'] = 1\n",
    "    else:\n",
    "        target.loc[row, 'label'] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_o = target[['id', 'label']]\n",
    "target_o.to_csv('./data/Predicts/output_ALBERT.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}